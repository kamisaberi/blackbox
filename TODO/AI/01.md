Yes, I have a very strong opinion on this.

**Do not try to train a "General Purpose" LLM (like Llama or BERT) for the core engine.**

Why?
1.  **Latency:** You are building a system for **100,000 Events Per Second (EPS)**. You have ~10 microseconds per log. A BERT model takes milliseconds. It will choke your pipeline immediately.
2.  **Relevance:** You don't need the AI to "write poetry" or "summarize text." You need it to classify **Patterns** (Numbers and Token Sequences).

### **The Strategy: "Small, Fast, Specialized"**

You should build a **Library of Micro-Models**, each specialized for a specific data type.

#### **Model 1: The Network Flow Autoencoder (Priority #1)**
*   **Target:** DDoS, Brute Force, Port Scans.
*   **Input:** Numerical data (Packet Size, Inter-arrival time, Flag counts).
*   **Architecture:** Dense Autoencoder (Feed-Forward).
*   **Why:** It is **Unsupervised**. You train it on "Normal Traffic" only. It learns what "Normal" looks like. Anything else (Zero-Day attacks) causes a high reconstruction error -> **Alert**.

#### **Model 2: The Log Sequence CNN (Priority #2)**
*   **Target:** SQL Injection, XSS, Shellcode in logs.
*   **Input:** Text (Apache/Nginx/Syslog lines).
*   **Architecture:** 1D-CNN (Convolutional Neural Network).
*   **Why:** CNNs are incredibly fast on GPUs (TensorRT loves them) and excellent at detecting "local patterns" (like `SELECT *` or `/etc/passwd`) regardless of where they appear in the string.

---

### **Action Plan: Building Model #1 (Network Flows)**

Let's build the **Network Flow Autoencoder** right now. This is the easiest to integrate with your current C++ `FeatureScaler`.

#### **1. The Dataset**
We will use **CIC-IDS2017**. It is the gold standard.
*   **Download:** You specifically want the **"MachineLearningCSV"** version (processed flows), not the raw PCAP.
*   **File:** `Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv` (Good mix of benign and attacks).

#### **2. The Training Script**

Create a new file: `blackbox-sim/scripts/train_network_flow.py`.

```python
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import joblib
import os

# --- 1. CONFIGURATION ---
DATA_FILE = "data/raw/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv"
ARTIFACTS_DIR = "data/artifacts"
os.makedirs(ARTIFACTS_DIR, exist_ok=True)

# Features to use (Must match what C++ parser extracts!)
# For MVP, we select 78 columns usually found in flow data
input_dim = 78 

# --- 2. MODEL DEFINITION ---
class FlowAutoencoder(nn.Module):
    def __init__(self):
        super(FlowAutoencoder, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.Tanh(),
            nn.Linear(64, 32),
            nn.Tanh(),
            nn.Linear(32, 16), # Bottleneck
            nn.Tanh()
        )
        self.decoder = nn.Sequential(
            nn.Linear(16, 32),
            nn.Tanh(),
            nn.Linear(32, 64),
            nn.Tanh(),
            nn.Linear(64, input_dim),
            nn.Sigmoid() # Because inputs are 0-1 scaled
        )

    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded

# --- 3. DATA LOADING ---
print(">>> Loading CSV...")
# Handle potential column name spaces
df = pd.read_csv(DATA_FILE)
df.columns = df.columns.str.strip()

# Replace Infinity/NaN
df.replace([np.inf, -np.inf], np.nan, inplace=True)
df.dropna(inplace=True)

# Separate Benign (Training) from Attacks (Testing)
benign_df = df[df['Label'] == 'BENIGN']
attack_df = df[df['Label'] != 'BENIGN']

# Drop Label column and other non-numeric strings
benign_data = benign_df.drop(columns=['Label', 'Destination Port', 'Flow ID', 'Source IP', 'Destination IP', 'Timestamp']).values
attack_data = attack_df.drop(columns=['Label', 'Destination Port', 'Flow ID', 'Source IP', 'Destination IP', 'Timestamp']).values

# Ensure 78 columns (padding or cutting) - vital for C++ compatibility
# In production, you select specific named columns.
benign_data = benign_data[:, :input_dim]
attack_data = attack_data[:, :input_dim]

print(f"    Benign Samples: {len(benign_data)}")
print(f"    Attack Samples: {len(attack_data)}")

# --- 4. SCALING ---
print(">>> Scaling...")
scaler = MinMaxScaler()
# Fit ONLY on benign data (We simulate 'normal' traffic)
scaler.fit(benign_data)

X_train = scaler.transform(benign_data)
X_test_attack = scaler.transform(attack_data)

# Save Scaler Params for C++
# Format: min,max per line
with open(f"{ARTIFACTS_DIR}/scaler_params.txt", "w") as f:
    for i in range(len(scaler.data_min_)):
        f.write(f"{scaler.data_min_[i]:.6f},{scaler.data_max_[i]:.6f}\n")
print("    Scaler params saved.")

# --- 5. TRAINING ---
print(">>> Training...")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = FlowAutoencoder().to(device)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)
batch_size = 256
epochs = 10

for epoch in range(epochs):
    model.train()
    permutation = torch.randperm(train_tensor.size(0))
    epoch_loss = 0.0
    
    for i in range(0, train_tensor.size(0), batch_size):
        indices = permutation[i:i+batch_size]
        batch = train_tensor[indices]
        
        optimizer.zero_grad()
        outputs = model(batch)
        loss = criterion(outputs, batch)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
        
    print(f"    Epoch {epoch+1}: Loss {epoch_loss/len(train_tensor):.6f}")

# --- 6. VALIDATION ---
print(">>> Validating...")
model.eval()
test_tensor = torch.tensor(X_test_attack[:1000], dtype=torch.float32).to(device) # Test 1000 attacks
with torch.no_grad():
    reconstruction = model(test_tensor)
    loss = torch.mean((test_tensor - reconstruction)**2, dim=1)
    avg_attack_score = loss.mean().item()

print(f"    Avg Reconstruction Error on Attacks: {avg_attack_score:.6f}")
print("    (If this is significantly higher than Training Loss, it works!)")

# --- 7. EXPORT ONNX ---
print(">>> Exporting ONNX...")
dummy_input = torch.randn(1, input_dim).to(device)
torch.onnx.export(
    model, 
    dummy_input, 
    f"{ARTIFACTS_DIR}/autoencoder.onnx",
    input_names=['input'], 
    output_names=['output'],
    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}
)
print("DONE.")
```

### **Step 3: Conversion to TensorRT (The "Speed" Step)**

Once `blackbox-sim` generates `autoencoder.onnx`, you should convert it to a TensorRT Engine (`.plan`) for maximum C++ performance.

You can do this inside your **Core Docker container** (since it has the NVIDIA tools installed):

```bash
# Inside blackbox-core container
/usr/src/tensorrt/bin/trtexec \
  --onnx=/app/config/autoencoder.onnx \
  --saveEngine=/app/models/autoencoder.plan \
  --fp16
```

**Why `--fp16`?**
This tells the GPU to use **Half-Precision** floating point numbers.
*   **Accuracy:** Almost identical to Float32.
*   **Speed:** 2x-3x faster.
*   **Memory:** 50% reduction.

### **The Workflow Summary**

1.  **Download CSV Data** -> `blackbox-sim/data/raw`.
2.  **Run `train_network_flow.py`** -> Generates `autoencoder.onnx` and `scaler_params.txt`.
3.  **Move Artifacts** -> To `blackbox-deploy/data/config`.
4.  **Launch Core** -> Core loads `scaler_params.txt`.
5.  **Convert Model** -> `trtexec` creates `.plan`.
6.  **Core Boots** -> Loads `.plan` and runs inference at 100k EPS.

This approach gives you a scientifically valid, high-performance model that fits perfectly into the architecture we built.